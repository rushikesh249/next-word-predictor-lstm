# ğŸ¤– Next Word Predictor LSTM

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://python.org)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.12.0-orange.svg)](https://tensorflow.org)
[![Flask](https://img.shields.io/badge/Flask-3.0.0-green.svg)](https://flask.palletsprojects.com)
[![OpenMP](https://img.shields.io/badge/OpenMP-Parallel-red.svg)](https://openmp.org)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A sophisticated **Next Word Prediction system** using LSTM neural networks with both web interface and high-performance C++ implementations. This project demonstrates the power of deep learning for natural language processing with real-time predictions and parallel computing optimizations.

## ğŸŒŸ Features

- **ğŸ§  LSTM Neural Network**: TensorFlow/Keras implementation with 128 hidden units
- **ğŸŒ Modern Web Interface**: Real-time predictions with beautiful UI
- **âš¡ OpenMP Optimization**: Parallel C++ implementation with 558x speedup
- **ğŸ“Š Performance Analysis**: Comprehensive timing and speedup metrics
- **ğŸ”„ Real-time Monitoring**: Backend connectivity and health checking
- **ğŸ“± Responsive Design**: Works on desktop, tablet, and mobile devices
- **ğŸ› ï¸ Easy Setup**: One-click startup with automated dependency management

## ğŸš€ Quick Start

### Option 1: Automated Setup (Recommended)
```bash
git clone https://github.com/yourusername/next-word-predictor-lstm.git
cd next-word-predictor-lstm
python scripts/start_project.py
```

### Option 2: Manual Setup
```bash
# Install dependencies
pip install -r requirements.txt

# Start backend server
python server.py

# Open frontend/index.html in your browser
```

## ğŸ¯ Demo

### Frontend Interface
![Frontend Interface](assets/frontend_interface.png)

### AI Prediction Results
![Prediction Results](assets/prediction_result.png)

### Try it live:
1. Enter text: "the dog"
2. Set word count: 2
3. Click "ğŸ”® Predict Next Words"
4. Get AI completion: "and i"

## ğŸ“ Project Structure

```
next-word-predictor-lstm/
â”œâ”€â”€ ğŸ“ frontend/           # Modern web interface
â”‚   â”œâ”€â”€ index.html        # Main HTML page
â”‚   â”œâ”€â”€ app.js           # JavaScript with real-time features
â”‚   â””â”€â”€ style.css        # Responsive CSS styling
â”œâ”€â”€ ğŸ“ cpp/              # High-performance C++ implementations
â”‚   â”œâ”€â”€ main.cpp         # Sequential implementation
â”‚   â”œâ”€â”€ openmp.cpp       # Parallel OpenMP version
â”‚   â””â”€â”€ timing.cpp       # Performance benchmarking
â”œâ”€â”€ ğŸ“ data/             # Training datasets
â”‚   â”œâ”€â”€ dataset_500.txt  # Small dataset (500 words)
â”‚   â”œâ”€â”€ dataset_1000.txt # Medium dataset (1K words)
â”‚   â”œâ”€â”€ dataset_8000.txt # Large dataset (8K words)
â”‚   â””â”€â”€ dataset_10000.txt# Full dataset (10K words)
â”œâ”€â”€ ğŸ“ scripts/          # Utility scripts
â”‚   â”œâ”€â”€ start_project.py # Automated startup
â”‚   â”œâ”€â”€ test_integration.py # Integration testing
â”‚   â””â”€â”€ graph.py         # Performance visualization
â”œâ”€â”€ ğŸ“ docs/             # Documentation
â”‚   â”œâ”€â”€ API.md           # API documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md  # System architecture
â”‚   â””â”€â”€ PPT_Content.txt  # Presentation materials
â”œâ”€â”€ ğŸ“ assets/           # Screenshots and media
â”œâ”€â”€ server.py            # Flask backend with LSTM
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ .gitignore          # Git ignore rules
â””â”€â”€ README.md           # This file
```

## ğŸ§  LSTM Architecture

```
Input (5 words) â†’ Embedding (64D) â†’ LSTM (128 units) â†’ Dense â†’ Softmax â†’ Prediction
```

**Model Specifications:**
- **Vocabulary Size**: 5,000 words
- **Sequence Length**: 5 words
- **Embedding Dimension**: 64
- **LSTM Units**: 128
- **Training Epochs**: 3 (configurable)
- **Optimizer**: Adam
- **Loss Function**: Sparse Categorical Crossentropy

## âš¡ Performance Results

OpenMP parallel implementation shows remarkable speedup:

| Dataset Size | Sequential | OpenMP | **Speedup** |
|-------------|-----------|---------|-------------|
| 500 words   | 0.68s     | 0.035s  | **19.4x**   |
| 1,000 words | 0.94s     | 0.076s  | **12.4x**   |
| 8,000 words | 7.18s     | 0.09s   | **79.8x**   |
| 10,000 words| 58.03s    | 0.104s  | **558x**    |

## ğŸ› ï¸ Technology Stack

### Backend
- **Python 3.9+** - Core programming language
- **Flask 3.0.0** - RESTful API framework
- **TensorFlow 2.12.0** - Deep learning framework
- **Keras** - High-level neural network API
- **NumPy 1.24.4** - Numerical computing

### Frontend
- **HTML5** - Modern markup
- **CSS3** - Responsive styling with animations
- **Vanilla JavaScript** - Real-time interactions
- **Fetch API** - Asynchronous backend communication

### C++ Implementation
- **C++11** - Modern C++ standards
- **OpenMP** - Parallel computing framework
- **STL** - Standard Template Library
- **Chrono** - High-precision timing

## ğŸ“– API Documentation

### Health Check
```http
GET /health
```
**Response:**
```json
{"status": "ok"}
```

### Predict Next Words
```http
POST /predict
Content-Type: application/json

{
  "text": "the quick brown",
  "num_words": 3
}
```
**Response:**
```json
{
  "completion": "fox jumps over",
  "words": ["fox", "jumps", "over"]
}
```

## ğŸ”§ Configuration

Environment variables for customization:

| Variable | Default | Description |
|----------|---------|-------------|
| `DATASET_FILE` | `data/dataset_10000.txt` | Training dataset path |
| `MAX_VOCAB` | `5000` | Maximum vocabulary size |
| `SEQ_LEN` | `5` | Input sequence length |
| `EPOCHS` | `3` | Training epochs |
| `LSTM_UNITS` | `128` | LSTM hidden units |
| `EMBED_DIM` | `64` | Embedding dimensions |

## ğŸ§ª Testing

```bash
# Run integration tests
python scripts/test_integration.py

# Test C++ implementations
cd cpp
g++ -std=c++11 -O2 -fopenmp openmp.cpp -o openmp
./openmp
```

## ğŸ“Š Visualization

Generate performance comparison charts:
```bash
python scripts/graph.py
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“ Educational Use

Perfect for:
- **Machine Learning Courses** - LSTM implementation and training
- **Web Development** - Full-stack application architecture
- **Parallel Computing** - OpenMP optimization techniques
- **Software Engineering** - Project structure and documentation

## ğŸš€ Future Enhancements

- [ ] **Transformer Models** - Implement attention mechanisms
- [ ] **GPU Acceleration** - CUDA support for faster training
- [ ] **Beam Search** - Multiple prediction candidates
- [ ] **User Authentication** - Personal prediction history
- [ ] **Model Fine-tuning** - Domain-specific adaptations
- [ ] **Mobile App** - React Native implementation

## ğŸ“ Support

If you encounter any issues or have questions:

1. Check the [documentation](docs/)
2. Search [existing issues](https://github.com/yourusername/next-word-predictor-lstm/issues)
3. Create a [new issue](https://github.com/yourusername/next-word-predictor-lstm/issues/new)

## â­ Show Your Support

Give a â­ï¸ if this project helped you!

---

**Built with â¤ï¸ for the AI and Machine Learning community**


